{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "projection method where data with m-columns (features) is projected into a subspace with m or fewer columns, whilst retaining the essence of the original data. <br>\n",
    "\n",
    "PCA is an operation applied to a dataset, represented by an n × m matrix A that results in a projection of A which we will call B <br>\n",
    "\n",
    "$$ A = \\begin{pmatrix}\n",
    "a_{1,1} & a_{1,2} \\\\\n",
    "a_{2,1} & a_{2,2} \\\\\n",
    "a_{3,1} & a_{3,2} \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$ B = PCA(A) $$\n",
    "\n",
    "first step is to calculate mean of column <br> \n",
    "\n",
    "$$ M = mean(A) $$\n",
    "\n",
    "Next, center the values in each column by subtracting the mean column value.<br>\n",
    "$$ C = A - M $$\n",
    "\n",
    "The next step is to calculate the covariance matrix of the centered matrix C. Correlation is a normalized measure of the amount and direction (positive or negative) that two columns change together. Covariance is a generalized and unnormalized version of correlation across multiple columns. A covariance matrix is a calculation of covariance of a given matrix with covariance scores for every column with every other column, including itself.\n",
    "\n",
    "<br> \n",
    "\n",
    "$$ V = cov(C) $$\n",
    "\n",
    "finally caluclate the eigendecomposition of covariance matrix V\n",
    "\n",
    "$$ values, vectors = eig(V) $$\n",
    "\n",
    "The eigenvectors represent the directions or components for the reduced subspace of B,\n",
    "whereas the eigenvalues represent the magnitudes for the directions. The eigenvectors can be\n",
    "sorted by the eigenvalues in descending order to provide a ranking of the components or axes of\n",
    "the new subspace for A. If all eigenvalues have a similar value, then we know that the existing\n",
    "representation may already be reasonably compressed or dense and that the projection may\n",
    "offer little. If there are eigenvalues close to zero, they represent components or axes of B that\n",
    "may be discarded. A total of m or less components must be selected to comprise the chosen\n",
    "subspace. Ideally, we would select k eigenvectors, called principal components, that have the k\n",
    "largest eigenvalues <br>\n",
    "\n",
    "$$ B = select(values,vectors) $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other matrix decomposition methods can be used such as Singular-Value Decomposition,\n",
    "or SVD. As such, generally the values are referred to as singular values and the vectors of the\n",
    "subspace are referred to as principal components. Once chosen, data can be projected into the\n",
    "subspace via matrix multiplication. <br>\n",
    "$$ P = B^T · A $$\n",
    "Where A is the original data that we wish to project, BT\n",
    "is the transpose of the chosen\n",
    "principal components and P is the projection of A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "[[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n",
      "[8. 0.]\n",
      "[[-2.82842712  0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 2.82842712  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#  define matrix\n",
    "A = np.array([\n",
    "[1, 2],\n",
    "[3, 4],\n",
    "[5, 6]])\n",
    "print(A)\n",
    "# column means\n",
    "M = np.mean(A.T, axis=1)\n",
    "# center columns by subtracting column means\n",
    "C = A - M\n",
    "# calculate covariance matrix of centered matrix\n",
    "V = np.cov(C.T)\n",
    "# factorize covariance matrix\n",
    "values, vectors = np.linalg.eig(V)\n",
    "print(vectors)\n",
    "print(values)\n",
    "# project data\n",
    "P = vectors.T.dot(C.T)\n",
    "print(P.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "[[ 0.70710678  0.70710678]\n",
      " [-0.70710678  0.70710678]]\n",
      "[8. 0.]\n",
      "[[-2.82842712e+00 -2.22044605e-16]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [ 2.82842712e+00  2.22044605e-16]]\n"
     ]
    }
   ],
   "source": [
    "# using scikit-learn\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "# define matrix\n",
    "A = np.array([\n",
    "[1, 2],\n",
    "[3, 4],\n",
    "[5, 6]])\n",
    "print(A)\n",
    "# create the transform\n",
    "pca = PCA(2)\n",
    "# fit transform\n",
    "pca.fit(A)\n",
    "# access values and vectors\n",
    "print(pca.components_)\n",
    "print(pca.explained_variance_)\n",
    "# transform data\n",
    "B = pca.transform(A)\n",
    "print(B)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34318cfe4829af221e5477cadc97d20e2fe9ea9ad002c04600364c092214ecf0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
